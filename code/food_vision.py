# -*- coding: utf-8 -*-
"""Food Vision.ipynb

Automatically generated by Colab.

#üçîüçú**Food Vision Project using Transfer Learning**üç©üçï</div>

#### **1. Problem Definition**
This project is based on working with 101 different classes of food to classify between each other.

#### **2. Goals**
The goal of this project is to beat the DeepFood, a 2016 paper which used a Convolutional Neural Network trained for 2-3 days to achieve 77.4% top-1 accuracy.

#### **3. Data**
For this project, the data has been downloaded from the `TensorFlow Datasets`.

#### **4. Evaluation**
The primary classification evaluation metric for this project is `accuracy`. However I will also use other metrics also to classify different food images.

#### **5. Model Architecture**
To beat the TOP-1 accuracy as per the goal, I have looked forward to working with the EfficientNetB0 (Convolutional Neural Network architecture) as it is giving pretty good results.

#### **6. Approach**
*   First, target dataset="food101" is downloaded from the tensorflow_datasets and splitted into the training and testing set.
*   Then I have made use of a technique named mixed precision as it helps in running model faster in training period and consumes less memory.
*   Then I have make use of feature extraction by freezing some of the layers of the model.
*   Then Fine Tuning comes  into play by unfreezing some of the layers to achieve greater results and accomplish my goal.
*   Afterwards other classification metrics came into use.
*   Then I have test my model on custom images.

### **Setting up the Initial Startup and Libraries**
"""

# Installing tensorflow
!pip install tensorflow==2.17.0

import tensorflow as tf
import keras

print(tf.__version__)
print(keras.__version__)

# Add timestamp
import datetime
print(f"Notebook last run (end-to-end): {datetime.datetime.now()}")

# Checking if GPU is available or not
physical_devices = tf.config.list_physical_devices('GPU')

if not physical_devices:
    print("No GPU detected.")
else:
    print("GPU detected:", physical_devices)

# Getting GPU name
!nvidia-smi

"""### **Downloading Food Vision dataset from tensorflow datasets**"""

# Get TensorFlow Datasets
import tensorflow_datasets as tfds

# List all available datasets
datasets_list = tfds.list_builders()     # get all available datasets in TFDS

# Set the target dataset and see if it exists
target_dataset = "food101"
print(f"'{target_dataset}' in TensorFlow Datasets: {target_dataset in datasets_list}")        # checking is target dataset in the list of TFDS datasets or not?

# Load in the data
(train_data, test_data), ds_info = tfds.load(name="food101",
                                             split=["train", "validation"],
                                             shuffle_files=False,
                                             as_supervised=True,      # data gets returned in tuple format (data, label)
                                             with_info=True)

# Features of Food101 from TFDS
ds_info.features

# Get the class names
class_names = ds_info.features["label"].names
class_names

"""### **Exploring the Food101 data from TensorFlow Datasets**

I want to find:
* Class names
* The shape of my input data (image tensors)
* The datatype of my input data
* What the labels look like (e.g. are they one-hot encoded or are they label encoded)
* Do the labels match up with the class names?
"""

# Take one sample of the train data
train_one_sample = train_data.take(1)         # samples are in format (image_tensor, label)
train_one_sample

# Output info about the training sample
for image, label in train_one_sample:
  print(f"""
  Image shape: {image.shape}
  Image datatype: {image.dtype}
  Target class from Food101 (tensor form): {label}
  Class name (str form): {class_names[label.numpy()]}
  """)

image

import tensorflow as tf
tf.reduce_min(image), tf.reduce_max(image)

"""### **Visualizing the visualizations**"""

# Plot an image tensor
import matplotlib.pyplot as plt
plt.imshow(image)
plt.title(class_names[label.numpy()])          # Add title to verify the label is associated with the right image
plt.axis(False);

fig = tfds.show_examples(train_data, ds_info)

"""### **Create preprocessing functions for the data**

Neural networks perform best when data is in a certain way (e.g. batched, normalized, etc).However, not all data (including data from TensorFlow Datasets) comes like this.

So in order to get it ready for a neural network, you'll often have to write preprocessing functions and map it to your data.

What we know about our data:
* In `uint8` datatype
* Comprised of all different size tensors (different sized images)
* Not scaled (the pixel values are between 0 and 255)

What we know models like:
* Data in `float32` dtype (or for mixed precision `float16` and `float32`)
* For batches, TensorFlow likes all of the tensors within a batch to be of the same size
* Scaled (values between 0 & 1) also called normalized tensors generally perform better

With these points in mind, we've got a few things we can tackle with a preprocessing function.

Since we're going to be using an EfficientNetBX pretrained model from tf.keras.applications we don't need to rescale our data (these architectures have rescaling built-in).

This means our function needs to:-
1. Reshape our images to all the same size
2. Convert the dtype of our image tensors from `uint8` to `float32`
"""

# Make a function for preprocessing images
def preprocess_img(image, label, img_shape=224):
  """
  Converts image datatype from 'uint8' -> 'float32' and reshapes image to (img_shape, img_shape, colour_channels)
  """
  image = tf.image.resize(image, [img_shape, img_shape])   # reshape target image
  # image = (image/255.)
  image = tf.cast(image, tf.float32)
  return image, label

"""### **Batch and prepare datasets**

For more resources on this, go through this guide: https://www.tensorflow.org/guide/data_performance
"""

# Map preprocessing function to training (and parallelize)
train_data = train_data.map(map_func=preprocess_img, num_parallel_calls = tf.data.AUTOTUNE)

# Turn train data into batches and prefetch it (load it faster)
train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)

# Map preprocessing function to test data
test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)

# Turn test data into batches ( don't need to shuffle)
test_data = test_data.batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)

# Print dataset shapes to verify
for images, labels in train_data.take(1):
    print("Train Data Shape:", images.shape)
for images, labels in test_data.take(1):
    print("Test Data Shape:", images.shape)

train_data, test_data

from tensorflow.keras import mixed_precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

from google.colab import drive
drive.mount('/content/drive')

"""### **Data Augmentation**

#### **Adding data augmentation right into the model**

To add data augmentation right into our models, I can make use of the layers inside:

* `tf.keras.layers.experimental.preprocessing()`

The benefits of using data augmentation inside the model are:
* Preprocessing of images (augmenting them) happens on the GPU (much faster) rather than the CPU.

* Image data augmentation only happens during training, so we can still export our whole model and use it elsewhere.
"""

import tensorflow as tf
from tensorflow.keras import layers

# Define data augmentation
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
    layers.RandomHeight(0.2),
    layers.RandomWidth(0.2),
    layers.Resizing(224, 224),
    # Rescaling should be uncommented if you want to normalize the image
    # layers.Rescaling(1/255.)
], name="data_augmentation")

"""#### **Visualizing the data augmentation layer**"""

# View a random image and compare it to its augmented version
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import os
import random

def visualize(original, augmented):
  fig = plt.figure()
  plt.subplot(1,2,1)
  plt.title('Original image')
  plt.imshow(original)
  plt.axis("off")

  plt.subplot(1,2,2)
  plt.title('Augmented image')
  plt.imshow(augmented)
  plt.axis("off")

flipped = tf.image.flip_left_right(image)
visualize(image, flipped)

# Add the image to a batch.
image = tf.cast(tf.expand_dims(image, 0), tf.float32)

plt.figure(figsize=(10, 10))
for i in range(9):
  augmented_image = data_augmentation(image)
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(augmented_image[0].numpy().astype("uint8"))
  plt.axis('off')

"""### **Creating modelling callbacks**

I am going to create a couple of callbacks to help us while my model trains:
* `TensorBoard callback` to log training results (so I can visualize them later if need be)
* `Model Checkpoint callback` to save my model's progress after feature extraction
"""

def create_tensorboard_callback(dir_name, experiment_name):
  """
  Creates a TensorBoard callback instand to store log files.

  Stores log files with the filepath:
    "dir_name/experiment_name/current_datetime/"

  Args:
    dir_name: target directory to store TensorBoard log files
    experiment_name: name of experiment directory (e.g. efficientnet_model_1)
  """
  log_dir = dir_name + "/" + experiment_name + "/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=log_dir
  )
  print(f"Saving TensorBoard log files to: {log_dir}")
  return tensorboard_callback

# Set EarlyStopping callback to stop training if model's val accuracy doesn't improve for 3 epochs
early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",
                                                  patience=3)

checkpoint_path = "/content/drive/MyDrive/Project: Food Vision/Feature_Extraction_model_checkpoints/cp.weights.h5"
model_checkpoints = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                       monitor="val_accuracy",
                                                       save_best_only=True,
                                                       save_weights_only=True,
                                                       verbose=0)

"""### **Transfer Learning**

#### **Feature Extraction**
"""

from tensorflow.keras import layers
import tensorflow as tf

# Setup input shape and base model, freezing the base model layers
input_shape = (224, 224, 3)
base_model = tf.keras.applications.EfficientNetB0(include_top=False, input_shape=input_shape)  # Specify input shape here
base_model.trainable = False

# Create input layer
inputs = layers.Input(shape=input_shape, name="input_layer")

# Add in data augmentation Sequential model as a layer
# x = data_augmentation(inputs)

# Give base_model the inputs (after augmentation) and don't train it
x = base_model(inputs, training=False)

# Pool output features of the base model
x = layers.GlobalAveragePooling2D(name="global_average_pooling_layer")(x)

x = layers.Dense(len(class_names))(x)
outputs = layers.Activation("softmax", dtype=tf.float32, name="softmax_float32")(x)

# Make a model using the inputs and outputs
model = tf.keras.Model(inputs, outputs)

# Let's take a look at the base model architecture
base_model.summary()

# Compile the model
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              metrics=["accuracy"])

history_feature_extraction = model.fit(train_data,
                                       epochs=3,
                                       steps_per_epoch=len(train_data),
                                       validation_data=test_data,
                                       validation_steps=int(0.1*len(test_data)),
                                       callbacks=[create_tensorboard_callback(dir_name="/content/drive/MyDrive/Project: Food Vision/Transfer Learning",
                                                                              experiment_name="Feature_extraction"),
                                                  model_checkpoints])

# Get the summary of model
model.summary()

# Evaluate on the test data
results_of_feature_extraction = model.evaluate(test_data)
results_of_feature_extraction

def plot_loss_curves(history):
  """
  Returns separate loss curves for training and validation metrics.
  """
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  epochs = range(len(history.history['loss']))

  # Plot loss
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label='training_accuracy')
  plt.plot(epochs, val_accuracy, label='val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();

plot_loss_curves(history_feature_extraction)

# Saving model
model.save("/content/drive/MyDrive/Project: Food Vision/Transfer Learning/Feature_extraction.keras")

"""#### **Loading in checkpointed weights**

Loading in checkpointed weights returns a model to a specific checkpoint.
"""

# Load in saved model
model.load_weights(checkpoint_path)

# Evaluate model with loaded weights
loaded_weighted_model_results = model.evaluate(test_data)

# If the results from our previously evaluated model match the loaded weights, everything has worked!
results_of_feature_extraction == loaded_weighted_model_results

# Checking to see if loaded model results are very close to my previous non-loaded model results
import numpy as np
np.isclose(np.array(results_of_feature_extraction), np.array(loaded_weighted_model_results))

# Checking the difference between the true results
print(np.array(results_of_feature_extraction) - np.array(loaded_weighted_model_results))

"""üìñ**Note**: Applying fine-tuning allows us to utilize pre-trained networks to recognize classes they were not originally trained on. And furthermore, this method can lead to higher accuracy than transfer learning via feature extraction.

#### **Fine Tuning**
"""

base_model.trainable = True

# adding regularization
regularizer = tf.keras.regularizers.l2(0.01)

for layer in base_model.layers:
    for attr in ['kernel_regularizer']:
        if hasattr(layer, attr):
          setattr(layer, attr, regularizer)

for layer in model.layers[1].layers:
    if isinstance(layer, layers.BatchNormalization):
        layer.trainable = False

for layer_number, layer in enumerate(model.layers[1].layers[-15:]):
    print(layer_number, layer.name, layer.trainable, layer.dtype, layer.dtype_policy)

# Set EarlyStopping callback to stop training if model's val accuracy doesn't improve for 3 epochs
early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",
                                                  patience=3)

# Create ModelCheckpoint callback to save best model during fine-tuning
checkpoint_path = "/content/drive/MyDrive/Project: Food Vision/Fine_Tuning_model_checkpoints/cp.weights.h5"
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                         monitor="val_accuracy",
                                                         save_best_only=True,
                                                         save_weights_only=True,
                                                         verbose=0)

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),
              metrics=['accuracy'])

len(model.trainable_variables)

history_fine_tune = model.fit(train_data,
                              epochs=10,
                              steps_per_epoch=len(train_data),
                              validation_data=test_data,
                              validation_steps=int(0.1*len(test_data)),
                              initial_epoch=history_feature_extraction.epoch[-1]+1,
                              callbacks=[create_tensorboard_callback(dir_name="/content/drive/MyDrive/Project: Food Vision/Transfer Learning",
                                                                              experiment_name="Fine_Tuning"),
                                         checkpoint_callback,
                                         early_stopping])

# Evaluate the model results
results_of_fine_tune = model.evaluate(test_data)
results_of_fine_tune

# Get the model summary
model.summary()

plot_loss_curves(history_fine_tune)

def compare_historys(original_history, new_history, initial_epochs):
    """
    Compares two TensorFlow model History objects.
    """

    # Get original history measurements
    acc = original_history.history["accuracy"]
    loss = original_history.history["loss"]

    val_acc = original_history.history["val_accuracy"]
    val_loss = original_history.history["val_loss"]

    # Combine original history with new history
    total_acc = acc + new_history.history["accuracy"]
    total_loss = loss + new_history.history["loss"]

    total_val_acc = val_acc + new_history.history["val_accuracy"]
    total_val_loss = val_loss + new_history.history["val_loss"]

    # Make plots
    plt.figure(figsize=(8, 8))
    plt.subplot(2, 1, 1)
    plt.plot(total_acc, label='Training Accuracy')
    plt.plot(total_val_acc, label='Validation Accuracy')
    plt.plot([initial_epochs-1, initial_epochs-1],
              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')

    plt.subplot(2, 1, 2)
    plt.plot(total_loss, label='Training Loss')
    plt.plot(total_val_loss, label='Validation Loss')
    plt.plot([initial_epochs-1, initial_epochs-1],
              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.xlabel('epoch')
    plt.show()

compare_historys(history_feature_extraction, history_fine_tune,initial_epochs=5)

# Saving model
model.save("/content/drive/MyDrive/Project: Food Vision/Transfer Learning/Fine_tuning.keras")

"""### **Model Predictions**"""

# Load in saved model
model.load_weights(checkpoint_path)

# Evaluate model with loaded weights
loaded_weighted_model_results_1 = model.evaluate(test_data)

# Make predictions with model
preds_probs = model.predict(test_data, verbose=1)

# How many predictions are there?
len(preds_probs)

# What's the shape of our predictions?
preds_probs.shape

# We get one prediction probability per class (in our case there's 101 prediction probabilities)
print(f"Number of prediction probabilities for sample 0: {len(preds_probs[0])}")
print(f"What prediction probability sample looks like:\n {preds_probs[0]}")
print(f"The class with the highest predicted probability by the model for sample 0: {preds_probs[0].argmax()}")

# Get the pred classes of each label
pred_classes = preds_probs.argmax(axis=1)

# How do they look?
pred_classes[:10]

# How many pred classes do we have?
len(pred_classes)

# To get our test labels we need to unravel our test_data BatchDataset
y_labels = []
for images, labels in test_data.unbatch():
  y_labels.append(labels.numpy())      # currently test labels look like: [0, 0, 0, 1, ... 0, 0], we want the index value where the "1" occurs
y_labels[:10]     # look at the first 10

# How many y_labels are there?
len(y_labels)

"""### **Evaluating my model's predictions**

One way to check our model's prediction array is in the same order as our test labels array is to find the accuracy score.
"""

loaded_weighted_model_results_1

# Let's try scikit-learn's accuracy sccore function and see what it comes up with
from sklearn.metrics import accuracy_score
sklearn_accuracy = accuracy_score(y_true=y_labels,
                                  y_pred=pred_classes)
sklearn_accuracy

# Does this metric come close to our model's evaluate results
import numpy as np
np.isclose(loaded_weighted_model_results_1[1], sklearn_accuracy)

"""#### **Other Classification evaluation metrics:-**

##### **Confusion Matrix**
"""

import itertools
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix

# Our function needs a different name to sklearn's plot_confusion_matrix
def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False):
  """Makes a labelled confusion matrix comparing predictions and ground truth labels.

  If classes is passed, confusion matrix will be labelled, if not, integer class values
  will be used.
  """
  # Create the confustion matrix
  cm = confusion_matrix(y_true, y_pred)
  cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] # normalize it
  n_classes = cm.shape[0] # find the number of classes we're dealing with

  # Plot the figure and make it pretty
  fig, ax = plt.subplots(figsize=figsize)
  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better
  fig.colorbar(cax)

  # Are there a list of classes?
  if classes:
    labels = classes
  else:
    labels = np.arange(cm.shape[0])

  # Label the axes
  ax.set(title="Confusion Matrix",
         xlabel="Predicted label",
         ylabel="True label",
         xticks=np.arange(n_classes), # create enough axis slots for each class
         yticks=np.arange(n_classes),
         xticklabels=labels, # axes will labeled with class names (if they exist) or ints
         yticklabels=labels)

  # Make x-axis labels appear on bottom
  ax.xaxis.set_label_position("bottom")
  ax.xaxis.tick_bottom()

  ### Changed (plot x-labels vertically) ###
  plt.xticks(rotation=90, fontsize=text_size)
  plt.yticks(fontsize=text_size)

  # Set the threshold for different colors
  threshold = (cm.max() + cm.min()) / 2.

  # Plot the text on each cell
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    if norm:
      plt.text(j, i, f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",
              horizontalalignment="center",
              color="white" if cm[i, j] > threshold else "black",
              size=text_size)
    else:
      plt.text(j, i, f"{cm[i, j]}",
              horizontalalignment="center",
              color="white" if cm[i, j] > threshold else "black",
              size=text_size)

  # Save the figure to the current working directory
  if savefig:
    fig.savefig("/content/drive/MyDrive/Project: Food Vision/confusion_matrix.png")

make_confusion_matrix(y_true=y_labels,
                      y_pred=pred_classes,
                      classes=class_names,
                      figsize=(100, 100),
                      text_size=20,
                      savefig=True)

"""##### **Classification Report**

Scikit-learn has a helpful function for acquiring many different classification metrics per class (e.g. precision, recall and F1 called classification_report, let's try it out.
"""

from sklearn.metrics import classification_report
print(classification_report(y_true=y_labels,
                           y_pred=pred_classes))

# Get a dictionary of the classification report
classification_report_dict = classification_report(y_labels, pred_classes, output_dict=True)
classification_report_dict

"""##### **F1-score**"""

# Create empty dictionary
class_f1_scores = {}

# Loop through classification report dictionary items
for k, v in classification_report_dict.items():
  if k == "accuracy":               # stop once we get to accuracy key
    break
  else:
    # Add class names and f1-score to new dictionary
    class_f1_scores[class_names[int(k)]] = v["f1-score"]
class_f1_scores

# Turn f1-scores into dataframe for visualization
import pandas as pd
f1_scores = pd.DataFrame({"class_names": list(class_f1_scores.keys()),
                          "f1-score": list(class_f1_scores.values())}).sort_values("f1-score", ascending=False)
f1_scores

import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(12,25))
scores = ax.barh(range(len(f1_scores)), f1_scores["f1-score"].values)       # get f1-score values
ax.set_yticks(range(len(f1_scores)))
ax.set_yticklabels(f1_scores["class_names"])
ax.set_xlabel("F1-score")
ax.set_title("F1-scores for 101 Different Food Classes (predicted by Food Vision mini)")
ax.invert_yaxis();  #reverse the order of our plot

def autolabel(rects):
  """
  Attach a text label above each bar displaying its height (it's value).
  """
  for rect in rects:
    width = rect.get_width()
    ax.text(1.03*width, rect.get_y() + rect.get_height()/1.5,
            f"{width:.2f}",
            ha='center', va='bottom')

autolabel(scores)

"""### **Visualizing predictions on custom images**

Now, this is the real test, how does my model go on food images not even in our test dataset.

To visualize my model's predictions on my own images, I am creating a function to load and preprocess images, specifically it will need to:

* Read in a target image filepath using tf.io.read_file()
* Turn the image into a Tensor using tf.io.decode_image()
* Resize the image tensor to be the same size as the images our model has trained on using tf.image.resize()
* Scale the image to get all of the pixel values between 0 and 1(if necessary).
"""

# Create a function to load and prepare images
def load_and_prep_image(filename, img_shape=224, scale=True):
  """
  Reads in an image from filename, turns it into a tensor and reshape into specified shape (img_shape, img_shape, color_channels=3).
  """
  # Read in the image
  img = tf.io.read_file(filename)

  # Decode image into tensor
  img = tf.io.decode_image(img, channels=3)

  # Resize the image
  img = tf.image.resize(img, [img_shape, img_shape])

  # Scale? Yes/no
  if scale:
    # rescale the image (get all values between 0 and 1)
    return img/255.
  else:
    return img       # don't need to rescale images for EfficientNet models in TensorFlow

"""Now we've got a function to load and prepare target images, let's now write some code to visualize images, their target label and our model's predictions.

Specifically, we'll write some code to:
1. Load a few random images from the test dataset
2. Make predictions on the loaded images
3. Plot the original image(s) along with the model's predictions, prediction probability and truth label
"""

cd /content/drive/MyDrive/Project: Food Vision

mkdir custom_food_images

# Get custom food images filepaths
custom_food_images = ["custom_food_images/" + img_path for img_path in os.listdir("custom_food_images")]
custom_food_images

# Make predictions on custom food images
for img in custom_food_images:
  img = load_and_prep_image(img, scale=False) # load in target image and turn it into tensor
  pred_prob = model.predict(tf.expand_dims(img, axis=0)) # make prediction on image with shape [None, 224, 224, 3]
  pred_class = class_names[pred_prob.argmax()] # find the predicted class label
  # Plot the image with appropriate annotations
  plt.figure()
  plt.imshow(img/255.) # imshow() requires float inputs to be normalized
  plt.title(f"pred: {pred_class}, prob: {pred_prob.max():.2f}")
  plt.axis(False)

